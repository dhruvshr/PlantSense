{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "MODELS = \"../models/\"\n",
    "\n",
    "print(os.path.exists(MODELS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "device = None\n",
    "device_count = 0\n",
    "cuda_available = torch.cuda.is_available()\n",
    "mps_available = torch.backends.mps.is_available()\n",
    "\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_count = torch.cuda.device_count()\n",
    "elif mps_available:\n",
    "    device = torch.device(\"mps\")\n",
    "    device_count = torch.mps.device_count()\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_count = torch.cpu.device_count()\n",
    "\n",
    "print(f\"Using device: {device}:{device_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load plantseg dataset\n",
    "\n",
    "PLANT_VILLAGE = \"../data/raw/Plant_leave_diseases_dataset_with_augmentation\"\n",
    "\n",
    "transforms = torchvision.transforms.Compose([\n",
    "    # Convert to tensor and normalize using ImageNet stats\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    # Resize images to consistent size\n",
    "    torchvision.transforms.Resize((224, 224)),\n",
    "    # Random augmentations for training\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "    torchvision.transforms.RandomRotation(degrees=30),\n",
    "    torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    # Random crop after resize for more robustness\n",
    "    torchvision.transforms.RandomResizedCrop(224, scale=(0.8, 1.0))\n",
    "])\n",
    "\n",
    "plant_village = torchvision.datasets.ImageFolder(PLANT_VILLAGE, transform=transforms)\n",
    "\n",
    "# plant_village_classes = plant_village.classes\n",
    "# plant_village_samples = plant_village.samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1538/1538 [03:59<00:00,  6.43batch/s, accuracy=50.1, loss=1.7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.6962, Accuracy: 50.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:28<00:00,  6.71batch/s, accuracy=52.8, loss=3.29]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.6374, Accuracy: 52.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1538/1538 [04:13<00:00,  6.06batch/s, accuracy=70.8, loss=0.935]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.9353, Accuracy: 70.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:30<00:00,  6.31batch/s, accuracy=69.5, loss=2.01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.0002, Accuracy: 69.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1538/1538 [04:10<00:00,  6.14batch/s, accuracy=78.7, loss=0.678]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.6776, Accuracy: 78.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:26<00:00,  7.18batch/s, accuracy=81.3, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5806, Accuracy: 81.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1538/1538 [04:19<00:00,  5.92batch/s, accuracy=82.5, loss=0.543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.5428, Accuracy: 82.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:28<00:00,  6.88batch/s, accuracy=80.9, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5839, Accuracy: 80.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1538/1538 [04:18<00:00,  5.94batch/s, accuracy=85.1, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.4539, Accuracy: 85.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:28<00:00,  6.77batch/s, accuracy=86.9, loss=0.83] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4133, Accuracy: 86.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1538/1538 [04:18<00:00,  5.94batch/s, accuracy=87.2, loss=0.391]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.3909, Accuracy: 87.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:30<00:00,  6.42batch/s, accuracy=88.4, loss=0.728]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3626, Accuracy: 88.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1538/1538 [04:19<00:00,  5.94batch/s, accuracy=88.8, loss=0.341]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.3414, Accuracy: 88.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:31<00:00,  6.07batch/s, accuracy=88.5, loss=0.691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3439, Accuracy: 88.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1538/1538 [04:27<00:00,  5.75batch/s, accuracy=90, loss=0.306]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.3057, Accuracy: 90.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:34<00:00,  5.56batch/s, accuracy=89.1, loss=0.67] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.3336, Accuracy: 89.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1538/1538 [04:53<00:00,  5.24batch/s, accuracy=90.9, loss=0.28] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.2798, Accuracy: 90.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:34<00:00,  5.62batch/s, accuracy=90.1, loss=0.582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2898, Accuracy: 90.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1538/1538 [04:27<00:00,  5.76batch/s, accuracy=91.6, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.2578, Accuracy: 91.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 193/193 [00:32<00:00,  5.97batch/s, accuracy=92.7, loss=0.449]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2236, Accuracy: 92.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 92.32%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:642] . invalid file name: ../models/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 125\u001b[0m\n\u001b[1;32m    121\u001b[0m     test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (test_correct \u001b[38;5;241m/\u001b[39m test_total)\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMODELS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/indexCodeWSL/PlantSense/.venv/lib/python3.12/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/indexCodeWSL/PlantSense/.venv/lib/python3.12/site-packages/torch/serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/indexCodeWSL/PlantSense/.venv/lib/python3.12/site-packages/torch/serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:642] . invalid file name: ../models/"
     ]
    }
   ],
   "source": [
    "# model init\n",
    "\n",
    "# split dataset into train, test, validation sets (80-10-10)\n",
    "dataset_size = len(plant_village)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "test_size = int(0.1 * dataset_size) \n",
    "val_size = dataset_size - train_size - test_size\n",
    "\n",
    "train, test, val = torch.utils.data.random_split(\n",
    "    plant_village, \n",
    "    [train_size, test_size, val_size]\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "\n",
    "# print(f\"Number of classes in train dataset: {len(train.dataset.classes)}\")\n",
    "# print(f\"Number of classes in train_loader dataset: {len(train_loader.dataset.dataset.classes)}\")\n",
    "\n",
    "\n",
    "# print(f\"Total dataset size: {dataset_size}\")\n",
    "# print(f\"Training set size: {len(train)}\")\n",
    "# print(f\"Test set size: {len(test)}\")\n",
    "# print(f\"Validation set size: {len(val)}\")\n",
    "\n",
    "\n",
    "# load preloaded resnet 18 cnn\n",
    "model = torchvision.models.resnet18()\n",
    "# print(model)\n",
    "\n",
    "# adding the fc layer to the model\n",
    "num_classes = len(train.dataset.classes)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# hyper parameters\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# train model\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with tqdm(train_loader, unit='batch', desc=f\"Epoch {epoch+1}/{epochs}\") as tepoch:\n",
    "        for i, (inputs, labels) in enumerate(tepoch):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # update the progress bar with the loss and accuracy\n",
    "            tepoch.set_postfix(loss=running_loss / (i + 1), accuracy=100 * correct / total)\n",
    "\n",
    "    # calculate accuracy\n",
    "    accuracy = 100 * (correct / total)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "\n",
    "        with tqdm(val_loader, unit='batch', desc=\"Validating\") as vepoch:\n",
    "            for (inputs, labels) in vepoch:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                valid_correct += (predicted == labels).sum().item()\n",
    "                valid_total += labels.size(0)\n",
    "\n",
    "                vepoch.set_postfix(loss=valid_loss / (valid_total / 64), accuracy=100 * valid_correct / valid_total)\n",
    "\n",
    "        valid_accuracy = 100 * (valid_correct / valid_total)\n",
    "        print(f\"Validation Loss: {valid_loss/len(val_loader):.4f}, Accuracy: {valid_accuracy:.2f}%\")\n",
    "        \n",
    "# test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "    for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "            test_total += labels.size(0)\n",
    "\n",
    "    test_accuracy = 100 * (test_correct / test_total)\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../models/modelv1_1.pth.\n"
     ]
    }
   ],
   "source": [
    "# Define the directory and filename\n",
    "model_dir = '../models/'\n",
    "os.makedirs(model_dir, exist_ok=True)  # Make sure the directory exists\n",
    "\n",
    "# Define the model save path\n",
    "model_save_path = os.path.join(model_dir, 'modelv1_1.pth')  # Adding a filename with extension\n",
    "\n",
    "# Save the model state_dict\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model checkpoint saved to ../models/modelv1_1.pth.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = model_save_path + '/checkpoints/'\n",
    "checkpoint_fname = f\"modelv1_1_chkpnt.pth\"\n",
    "\n",
    "chkpnt_path = \"../models/checkpoints/modelv1_1_checkpoint.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'criterion_state_dict': criterion.state_dict(),\n",
    "    'epoch': epoch,\n",
    "    'loss': loss,\n",
    "}, chkpnt_path)\n",
    "print(f\"Model checkpoint saved to {model_save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model for some ruf inference\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
